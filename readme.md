# Topic-aware Demonstrations for Open-domain Question Expansion

## Installation

Installation from the source.

```
git clone git@github.com:XD-BDIV-NLP/TDPR.git
cd TDPR
pip install .
```

We follow the work of [DPR](https://github.com/facebookresearch/DPR), using Python 3.6+ and PyTorch 1.2.0+.

## Prepare data

First, extract the questions from the dataset into a TSV file, making it easy to feed into DPR to obtain the DPR vector representation of the questions.

```
python data_process_main.py \
--origin_train_data_path={the origin json train datasets path} \
--origin_dev_data_path={the origin json dev datasets path} \
--origin_test_data_path={the origin csv test datasets path} \
--train_tsv_save_path={tsv train data save path} \
--dev_tsv_save_path={tsv dev data save path} \
--test_tsv_save_path={tsv test data save path}
```

After obtaining the list of questions, we follow the example of `dpr_wiki` to feed them into DPR to obtain their corresponding embedding representations.

## K-means

After obtaining the list of questions, while feeding them into DPR, we also obtain their vector representations using BERT to facilitate the calculation of their k-means clustering:

```
python bert_cls_main.py \
--question_path [your TSV file here] \
--save_path [save path here]
```

Next, we compute the [k-means](https://github.com/subhadarship/kmeans_pytorch) for the training set, validation set, and test set together. Note that we first calculate the k-means clustering for the training set,  the questions in the validation and test sets will be assigned to the nearest centroid of the training set clusters.

```
python kmeans_main.py \
--train_cls_path [train path] \
--test_cls_path [test path] \
--dev_cls_path [dev path] \
--num_clusters [we use 30 in nq] \
--output_path [train save] \
--test_save_path [test save] \
--dev_save_path [dev save]
```

## Generate pseudo passages

After obtaining the list of clusters, we will generate pseudo-passages with topic awareness, using articles of the same theme to help the larger model better generate pseudo-passages.

```
python get_d_plus_main.py \
--origin_train_file={your origin train datasets path} \
--train_cluster_id_file={your train datasets' cluster id file path} \
--origin_raw_file={your datasets to generate the pseudo passages path} \
--type={using train/dev/test, i.e., your origin_raw_file type} \
--crp_cluster_id_file={your origin_raw_file's corresponding cluster id file} \
--num_clusters={the num you choose during k-means} \
--kmeans={using zero_shot, random, kmeans_random or topic} \
--output_dir={pseudo passages' output path}
```

Noting that whether the origin_raw_file is train/dev/test, the origin_train_file is always the train datasets. Because during k-means, we only calculate the k-means clustering for the training set.

We also provide a method for generating pseudo-articles using LLaMA, as seen in `llama.py`.

## Generate keywords

We use bert-base-uncased to parse the embeddings generated by DPR and to extract keywords. After parsing, we also remove stopwords and words that have already appeared in the questions/pseudo-passages.

```
python filter_hidden_main.py \
--dpr_file_path [dpr vector] \
--origin_file_path [your origin train/dev/test file] \
--d_plus_path [pseudo passage file] \
--output_path [save path] \
--is_test [whether is using test set or not]
```

Finally, we concatenate the original questions, pseudo-passages, and keywords to form new questions, which are then written back into the original dataset.

```
python concat_save_main.py \
--hidden_path [keywords file] \
--d_file_path [pseudo passage file] \
--origin_file_path [your origin train/dev/test file] \
--write_path [save path] \
--concat [to concat both keywords and pseudo passage or just keywords/pseudo passage alone. Using both/keyword/d_plus] \
--is_test [whether is using test set or not]
```

## Training

The training section is conducted with reference to the training section of [DPR](https://github.com/facebookresearch/DPR).



## Checkpoints

We have released our trained checkpoints at the following link.

[Four datasets TDPR checkpoints](https://pan.baidu.com/s/1-PxjOrxjsqkiZhiPQmQrSw?pwd=l2ux)